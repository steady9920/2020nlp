{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from gensim import models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('friends_train.json',encoding='cp1252') as f:\n",
    "  emotion_train = json.load(f)\n",
    "\n",
    "with open('friends_dev.json',encoding='cp1252') as f:\n",
    "  emotion_dev = json.load(f)\n",
    "\n",
    "with open('friends_test.json',encoding='cp1252') as f:\n",
    "  emotion_test = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(columns=['i_dialog','i_utterance','speaker','utterance','emotion'])\n",
    "test_data = pd.DataFrame(columns=['i_dialog','i_utterance','speaker','utterance','emotion'])\n",
    "kaggle_data = pd.read_csv('en_data.csv',encoding='cp1252')\n",
    "\n",
    "for i_dialog,scen in enumerate(emotion_train):    \n",
    "    for i_utterance,dialog in enumerate(scen):        \n",
    "        train_data = train_data.append({'i_dialog':i_dialog,'i_utterance':i_utterance,'speaker':dialog['speaker'],'utterance':dialog['utterance'],'emotion':dialog['emotion']}, ignore_index=True)\n",
    "\n",
    "for i_dialog,scen in enumerate(emotion_dev):    \n",
    "    for i_utterance,dialog in enumerate(scen):        \n",
    "        train_data = train_data.append({'i_dialog':i_dialog,'i_utterance':i_utterance,'speaker':dialog['speaker'],'utterance':dialog['utterance'],'emotion':dialog['emotion']}, ignore_index=True)\n",
    "\n",
    "\n",
    "for i_dialog,scen in enumerate(emotion_test):\n",
    "    # before_utterance = ''\n",
    "    for i_utterance,dialog in enumerate(scen):        \n",
    "        test_data = test_data.append({'i_dialog':i_dialog,'i_utterance':i_utterance,'speaker':dialog['speaker'],'utterance':dialog['utterance'],'emotion':dialog['emotion']}, ignore_index=True)\n",
    "        # test_data = test_data.append({'i_dialog':i_dialog,'i_utterance':i_utterance,'speaker':dialog['speaker'],'utterance':before_utterance+' '+dialog['utterance'],'emotion':dialog['emotion']}, ignore_index=True)\n",
    "        # before_utterance = dialog['utterance']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for row in train_data['utterance']:   \n",
    "    X_train.append(gensim.utils.simple_preprocess(row))\n",
    "\n",
    "\n",
    "X_test = []\n",
    "for row in test_data['utterance']:   \n",
    "    X_test.append(gensim.utils.simple_preprocess(row))\n",
    "\n",
    "\n",
    "X_kaggle = []\n",
    "for row in kaggle_data['utterance']:   \n",
    "    X_kaggle.append(gensim.utils.simple_preprocess(row))    \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for a in train_data['emotion']:\n",
    "    if a=='neutral':\n",
    "        y_train = np.append(y_train, 1 )        \n",
    "    elif a=='joy':\n",
    "        y_train = np.append(y_train, 2)\n",
    "    elif a=='sadness':\n",
    "        y_train = np.append(y_train, 3)\n",
    "    elif a=='fear':\n",
    "        y_train = np.append(y_train, 4)\n",
    "    elif a=='anger':\n",
    "        y_train = np.append(y_train, 5)\n",
    "    elif a=='suprise':\n",
    "        y_train = np.append(y_train, 6)\n",
    "    elif a=='disgust':\n",
    "        y_train = np.append(y_train, 7)\n",
    "    elif a=='non-neutral':\n",
    "        y_train = np.append(y_train, 8)\n",
    "    elif a=='surprise':\n",
    "        y_train = np.append(y_train, 0)\n",
    "y_train = to_categorical(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "for a in test_data['emotion']:\n",
    "    if a=='neutral':\n",
    "        y_test = np.append(y_test, 1 )        \n",
    "    elif a=='joy':\n",
    "        y_test = np.append(y_test, 2)\n",
    "    elif a=='sadness':\n",
    "        y_test = np.append(y_test, 3)\n",
    "    elif a=='fear':\n",
    "        y_test = np.append(y_test, 4)\n",
    "    elif a=='anger':\n",
    "        y_test = np.append(y_test, 5)\n",
    "    elif a=='suprise':\n",
    "        y_test = np.append(y_test, 6)\n",
    "    elif a=='disgust':\n",
    "        y_test = np.append(y_test, 7)\n",
    "    elif a=='non-neutral':\n",
    "        y_test = np.append(y_test, 8)\n",
    "    elif a=='surprise':\n",
    "        y_test = np.append(y_test, 0)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "리뷰의 최대 길이 : 65\n리뷰의 평균 길이 : 7.389896924780646\n"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "전체 샘플 중 길이가 22 이하인 샘플의 비율: 98.06627481046085\n"
    }
   ],
   "source": [
    "max_len = 22\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 15))\n",
    "model.add(LSTM(128))\n",
    "#model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "#model.add(Dense(9, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/30\n91/92 [============================>.] - ETA: 0s - loss: 1.6770 - acc: 0.4451\nEpoch 00001: val_acc improved from -inf to 0.46563, saving model to best_model.h5\n92/92 [==============================] - 4s 45ms/step - loss: 1.6758 - acc: 0.4450 - val_loss: 1.5702 - val_acc: 0.4656\nEpoch 2/30\n92/92 [==============================] - ETA: 0s - loss: 1.5900 - acc: 0.4467\nEpoch 00002: val_acc improved from 0.46563 to 0.46599, saving model to best_model.h5\n92/92 [==============================] - 3s 38ms/step - loss: 1.5900 - acc: 0.4467 - val_loss: 1.5497 - val_acc: 0.4660\nEpoch 3/30\n91/92 [============================>.] - ETA: 0s - loss: 1.5135 - acc: 0.4609\nEpoch 00003: val_acc improved from 0.46599 to 0.48770, saving model to best_model.h5\n92/92 [==============================] - 3s 38ms/step - loss: 1.5138 - acc: 0.4612 - val_loss: 1.4956 - val_acc: 0.4877\nEpoch 4/30\n91/92 [============================>.] - ETA: 0s - loss: 1.3992 - acc: 0.5058\nEpoch 00004: val_acc did not improve from 0.48770\n92/92 [==============================] - 3s 37ms/step - loss: 1.3995 - acc: 0.5058 - val_loss: 1.4553 - val_acc: 0.4805\nEpoch 5/30\n92/92 [==============================] - ETA: 0s - loss: 1.2991 - acc: 0.5443\nEpoch 00005: val_acc did not improve from 0.48770\n92/92 [==============================] - 3s 38ms/step - loss: 1.2991 - acc: 0.5443 - val_loss: 1.4663 - val_acc: 0.4859\nEpoch 6/30\n91/92 [============================>.] - ETA: 0s - loss: 1.2193 - acc: 0.5729\nEpoch 00006: val_acc did not improve from 0.48770\n92/92 [==============================] - 3s 37ms/step - loss: 1.2191 - acc: 0.5728 - val_loss: 1.4650 - val_acc: 0.4844\nEpoch 7/30\n91/92 [============================>.] - ETA: 0s - loss: 1.1441 - acc: 0.6007\nEpoch 00007: val_acc did not improve from 0.48770\n92/92 [==============================] - 3s 37ms/step - loss: 1.1434 - acc: 0.6010 - val_loss: 1.4905 - val_acc: 0.4768\nEpoch 8/30\n92/92 [==============================] - ETA: 0s - loss: 1.0970 - acc: 0.6222\nEpoch 00008: val_acc did not improve from 0.48770\n92/92 [==============================] - 3s 37ms/step - loss: 1.0970 - acc: 0.6222 - val_loss: 1.5255 - val_acc: 0.4873\nEpoch 00008: early stopping\n"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "87/87 [==============================] - 1s 6ms/step - loss: 1.4956 - acc: 0.4877\n\n 테스트 정확도: 0.4877\n"
    }
   ],
   "source": [
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_kaggle = []\n",
    "for row in kaggle_data['utterance']:   \n",
    "    X_kaggle.append(gensim.utils.simple_preprocess(row))    \n",
    "X_kaggle = tokenizer.texts_to_sequences(X_kaggle)\n",
    "X_kaggle = pad_sequences(X_kaggle, maxlen = max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_result = pd.DataFrame(columns=['Id','Predicted'])\n",
    "i=0\n",
    "for p in loaded_model.predict(X_kaggle):\n",
    "    emotion = np.argmax(p)\n",
    "    if emotion == 1:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'neutral'}, ignore_index=True)\n",
    "    elif emotion == 2: \n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'joy'}, ignore_index=True)\n",
    "    elif emotion == 3:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'sadness'}, ignore_index=True)\n",
    "    elif emotion == 4:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'fear'}, ignore_index=True)\n",
    "    elif emotion == 5:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'anger'}, ignore_index=True)\n",
    "    elif emotion == 6:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'suprise'}, ignore_index=True)\n",
    "    elif emotion == 7:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'disgust'}, ignore_index=True)\n",
    "    elif emotion == 8:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'non-neutral'}, ignore_index=True)\n",
    "    elif emotion == 0:\n",
    "        kaggle_result = kaggle_result.append({'Id':i,'Predicted':'surprise'}, ignore_index=True)\n",
    "    i=i+1\n",
    "\n",
    "kaggle_result.to_csv('2조.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitcdf7bd633ba94d1aa92e033e46ea72ab",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}