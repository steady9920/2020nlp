{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train/test data\n",
    "\n",
    "using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글을 제외한 문자를 regual express 을 이용하여 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data = test_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석\n",
    "\n",
    "konlpy 을 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "\n",
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = 19416\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "x_train = tokenizer.texts_to_sequences(X_train)\n",
    "x_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) < 1]\n",
    "\n",
    "x_train = np.delete(x_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "x_test = np.delete(x_test, drop_test, axis=0)\n",
    "y_test = np.delete(y_test, drop_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "x_train = pad_sequences(x_train, maxlen = max_len)\n",
    "x_test = pad_sequences(x_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/15\n1916/1916 [==============================] - ETA: 0s - loss: 0.3907 - acc: 0.8225\nEpoch 00001: val_acc improved from -inf to 0.84313, saving model to best_model.h5\n1916/1916 [==============================] - 102s 53ms/step - loss: 0.3907 - acc: 0.8225 - val_loss: 0.3550 - val_acc: 0.8431\nEpoch 2/15\n1915/1916 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8560\nEpoch 00002: val_acc improved from 0.84313 to 0.85305, saving model to best_model.h5\n1916/1916 [==============================] - 104s 54ms/step - loss: 0.3299 - acc: 0.8559 - val_loss: 0.3374 - val_acc: 0.8531\nEpoch 3/15\n1915/1916 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.8706\nEpoch 00003: val_acc improved from 0.85305 to 0.85559, saving model to best_model.h5\n1916/1916 [==============================] - 107s 56ms/step - loss: 0.3040 - acc: 0.8706 - val_loss: 0.3306 - val_acc: 0.8556\nEpoch 4/15\n1915/1916 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.8812\nEpoch 00004: val_acc did not improve from 0.85559\n1916/1916 [==============================] - 101s 53ms/step - loss: 0.2848 - acc: 0.8812 - val_loss: 0.3413 - val_acc: 0.8538\nEpoch 5/15\n1916/1916 [==============================] - ETA: 0s - loss: 0.2684 - acc: 0.8898\nEpoch 00005: val_acc improved from 0.85559 to 0.85747, saving model to best_model.h5\n1916/1916 [==============================] - 103s 54ms/step - loss: 0.2684 - acc: 0.8898 - val_loss: 0.3293 - val_acc: 0.8575\nEpoch 6/15\n1915/1916 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8972\nEpoch 00006: val_acc did not improve from 0.85747\n1916/1916 [==============================] - 103s 54ms/step - loss: 0.2531 - acc: 0.8972 - val_loss: 0.3353 - val_acc: 0.8564\nEpoch 7/15\n1915/1916 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9047\nEpoch 00007: val_acc did not improve from 0.85747\n1916/1916 [==============================] - 99s 52ms/step - loss: 0.2377 - acc: 0.9047 - val_loss: 0.3595 - val_acc: 0.8470\nEpoch 8/15\n1915/1916 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9119\nEpoch 00008: val_acc did not improve from 0.85747\n1916/1916 [==============================] - 94s 49ms/step - loss: 0.2218 - acc: 0.9119 - val_loss: 0.3655 - val_acc: 0.8445\nEpoch 9/15\n1915/1916 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9201\nEpoch 00009: val_acc did not improve from 0.85747\n1916/1916 [==============================] - 94s 49ms/step - loss: 0.2048 - acc: 0.9201 - val_loss: 0.3637 - val_acc: 0.8491\nEpoch 00009: early stopping\n"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1513/1513 [==============================] - 14s 9ms/step - loss: 0.3340 - acc: 0.8559\n\n 테스트 정확도: 0.8559\n"
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitcdf7bd633ba94d1aa92e033e46ea72ab",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}